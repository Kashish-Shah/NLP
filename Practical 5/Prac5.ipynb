{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cba00bc",
   "metadata": {},
   "source": [
    "# Prac 5:Topic Modelling using LSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1abde370",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "import seaborn as sns\n",
    "#configure\n",
    "# sets matplotlib to inline and displays graphs below the corressponding cell.\n",
    "%matplotlib inline  \n",
    "style.use('fivethirtyeight')\n",
    "sns.set(style='whitegrid',color_codes=True)\n",
    "\n",
    "#import nltk\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "\n",
    "#preprocessing\n",
    "from nltk.corpus import stopwords  #stopwords\n",
    "from nltk import word_tokenize,sent_tokenize # tokenizing\n",
    "from nltk.stem import PorterStemmer,LancasterStemmer  # using the Porter Stemmer and Lancaster Stemmer and others\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer  # lammatizer from WordNet\n",
    "\n",
    "# for named entity recognition (NER)\n",
    "from nltk import ne_chunk\n",
    "\n",
    "# vectorizers for creating the document-term-matrix (DTM)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "\n",
    "#stop-words\n",
    "stop_words=set(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9bfc23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(r'abcnews-date-text.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a76efc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the publish date.\n",
    "df.drop(['publish_date'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68489397",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(headline):\n",
    "  le=WordNetLemmatizer()\n",
    "  word_tokens=word_tokenize(headline)\n",
    "  tokens=[le.lemmatize(w) for w in word_tokens if w not in stop_words and len(w)>3]\n",
    "  cleaned_text=\" \".join(tokens)\n",
    "  return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c468e3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['headline_cleaned_text']=df['headline_text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4638134f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['headline_text'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1567fe08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline_cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>decides community broadcasting licence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fire witness must aware defamation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>call infrastructure protection summit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>staff aust strike rise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>strike affect australian traveller</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    headline_cleaned_text\n",
       "0  decides community broadcasting licence\n",
       "1      fire witness must aware defamation\n",
       "2   call infrastructure protection summit\n",
       "3                  staff aust strike rise\n",
       "4      strike affect australian traveller"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09563e9f",
   "metadata": {},
   "source": [
    "# Extracting the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72047842",
   "metadata": {},
   "outputs": [],
   "source": [
    "vect =TfidfVectorizer(stop_words=stop_words,max_features=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "767dac2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vect_text=vect.fit_transform(df['headline_cleaned_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f7a6137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1093281, 1000)\n",
      "  (0, 507)\t0.7808664326566723\n",
      "  (0, 178)\t0.6246980185257855\n",
      "  (1, 575)\t0.6344261126788409\n",
      "  (1, 982)\t0.6340174024214789\n",
      "  (1, 320)\t0.44218258782762476\n",
      "  (2, 683)\t0.8255691240254475\n",
      "  (2, 120)\t0.5643010025295501\n",
      "  (3, 745)\t0.4555401202137725\n",
      "  (3, 846)\t0.48936875561594206\n",
      "  (3, 52)\t0.5178741905948874\n",
      "  (3, 830)\t0.533673816687887\n",
      "  (4, 54)\t0.6508194560051154\n",
      "  (4, 846)\t0.759232530707955\n",
      "  (5, 472)\t0.7947699014549592\n",
      "  (5, 977)\t0.6069108696845646\n",
      "  (6, 711)\t1.0\n",
      "  (7, 543)\t0.5164389518088216\n",
      "  (7, 343)\t0.4823478536254996\n",
      "  (7, 960)\t0.5398043943633849\n",
      "  (7, 51)\t0.4574304023383868\n",
      "  (8, 452)\t0.4263247344974713\n",
      "  (8, 200)\t0.34763893401982066\n",
      "  (8, 778)\t0.450702975997798\n",
      "  (8, 12)\t0.5264434547215587\n",
      "  (8, 52)\t0.4659597715648655\n",
      "  :\t:\n",
      "  (1093273, 393)\t0.5033599258208994\n",
      "  (1093274, 49)\t0.6289811610547521\n",
      "  (1093274, 830)\t0.7774205419451019\n",
      "  (1093275, 945)\t0.513993466209999\n",
      "  (1093275, 549)\t0.4147189949636653\n",
      "  (1093275, 877)\t0.4333914165329414\n",
      "  (1093275, 805)\t0.42297946751737486\n",
      "  (1093275, 43)\t0.4439359436270581\n",
      "  (1093276, 945)\t0.48622609701483094\n",
      "  (1093276, 877)\t0.40997839621262017\n",
      "  (1093276, 805)\t0.40012892989647064\n",
      "  (1093276, 768)\t0.5067197810076478\n",
      "  (1093276, 778)\t0.4226509200464596\n",
      "  (1093277, 970)\t0.7511209001160061\n",
      "  (1093277, 54)\t0.6601646714335149\n",
      "  (1093278, 924)\t0.39673422155963234\n",
      "  (1093278, 503)\t0.4374210644720661\n",
      "  (1093278, 254)\t0.4378313656165735\n",
      "  (1093278, 599)\t0.41541772005836836\n",
      "  (1093278, 421)\t0.340533154966081\n",
      "  (1093278, 973)\t0.4135622725929888\n",
      "  (1093279, 716)\t0.6910413688032163\n",
      "  (1093279, 122)\t0.722815209166615\n",
      "  (1093280, 671)\t0.7689410367061928\n",
      "  (1093280, 988)\t0.6393197025504576\n"
     ]
    }
   ],
   "source": [
    "print(vect_text.shape)\n",
    "print(vect_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "664b8929",
   "metadata": {},
   "outputs": [],
   "source": [
    "idf=vect.idf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17535524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "police shop\n",
      "4.423470857785716\n",
      "7.938670552264349\n"
     ]
    }
   ],
   "source": [
    "dd=dict(zip(vect.get_feature_names(), idf))\n",
    "l=sorted(dd, key=(dd).get)\n",
    "# print(l)\n",
    "print(l[0],l[-1])\n",
    "print(dd['police'])\n",
    "print(dd['forecast'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fc2f03",
   "metadata": {},
   "source": [
    "# Latent Semantic Analysis (LSA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f3631b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "lsa_model = TruncatedSVD(n_components=10, algorithm='randomized', n_iter=10, random_state=42)\n",
    "\n",
    "lsa_top=lsa_model.fit_transform(vect_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d4c0af78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 9.71372990e-05  9.74621196e-03  2.40828560e-02 ... -8.73837587e-05\n",
      "  -2.76001881e-03 -4.71492128e-04]\n",
      " [ 5.38024755e-04  5.09018771e-02  8.05111623e-02 ... -4.44691202e-02\n",
      "   2.69886897e-02  1.23102458e-03]\n",
      " [ 5.30903781e-04  4.84053526e-02  1.49968429e-01 ... -7.74930642e-02\n",
      "  -7.00245363e-04 -3.97849327e-02]\n",
      " ...\n",
      " [ 1.68011206e-03  1.42960980e-02  2.22267421e-02 ... -1.02617038e-03\n",
      "   3.58910841e-03 -1.39351363e-03]\n",
      " [ 7.31404460e-05  4.17196796e-03  1.01954634e-02 ...  6.16221894e-03\n",
      "   1.34376432e-04 -1.05918795e-03]\n",
      " [ 1.53872057e-04  1.26988867e-02  3.50453244e-02 ...  1.47034561e-03\n",
      "   2.08891226e-04  7.21618681e-03]]\n",
      "(1093281, 10)\n"
     ]
    }
   ],
   "source": [
    "print(lsa_top)\n",
    "print(lsa_top.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6d6c48d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 0 :\n",
      "Topic  0  :  0.00971372990006908\n",
      "Topic  1  :  0.9746211955444511\n",
      "Topic  2  :  2.4082856047676215\n",
      "Topic  3  :  -0.7016646013301822\n",
      "Topic  4  :  -0.2249388745261046\n",
      "Topic  5  :  -0.011775256729257678\n",
      "Topic  6  :  0.05392210930771517\n",
      "Topic  7  :  -0.008738375866253627\n",
      "Topic  8  :  -0.27600188100988293\n",
      "Topic  9  :  -0.047149212813676684\n"
     ]
    }
   ],
   "source": [
    "l=lsa_top[0]\n",
    "print(\"Document 0 :\")\n",
    "for i,topic in enumerate(l):\n",
    "  print(\"Topic \",i,\" : \",topic*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cc7631bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: \n",
      "interview extended michael john david smith james andrew mark police \n",
      "\n",
      "Topic 1: \n",
      "police death probe fire woman crash call court missing drug \n",
      "\n",
      "Topic 2: \n",
      "say plan council call govt back fire australia water court \n",
      "\n",
      "Topic 3: \n",
      "say australia police report need minister must world could expert \n",
      "\n",
      "Topic 4: \n",
      "court fire face woman murder charged charge accused death crash \n",
      "\n",
      "Topic 5: \n",
      "call medium say australia inquiry report prompt spark change opposition \n",
      "\n",
      "Topic 6: \n",
      "fire house home govt crew say blaze damage threat school \n",
      "\n",
      "Topic 7: \n",
      "australia back world south first australian test take win india \n",
      "\n",
      "Topic 8: \n",
      "council australia crash fire rate year rise death seek dy \n",
      "\n",
      "Topic 9: \n",
      "back report council fight market rural hit business news police \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# most important words for each topic\n",
    "vocab = vect.get_feature_names()\n",
    "\n",
    "for i, comp in enumerate(lsa_model.components_):\n",
    "    vocab_comp = zip(vocab, comp)\n",
    "    sorted_words = sorted(vocab_comp, key= lambda x:x[1], reverse=True)[:10]\n",
    "    print(\"Topic \"+str(i)+\": \")\n",
    "    for t in sorted_words:\n",
    "        print(t[0],end=\" \")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024e12db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
